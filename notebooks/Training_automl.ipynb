{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"The file is used for training automl models for forecasting. \n",
    "The file requires a config file (e.g. model_run.json) that has arguments\n",
    "corresponding to the different member variables of the below classes.\n",
    "The config file is specified in the variable filepath (refer end of the program)\n",
    "When the notebook begins running, if training succeeds the cell output will be of the form\n",
    "\"PIPELINE_STATE_RUNNING\". You will also be able to verify if the model training through the cloud GUI\n",
    "in the section Vertex AI -> Training.\n",
    "The final trained model is stored at Vertex AI -> Models.\n",
    "For additional docs specifically for forecasting: https://drive.google.com/drive/folders/1Uj0mFA-u9_g1jHYxzy8zqUVDtdQuNxPw?usp=sharing\n",
    "For docs related to API for the functions used below: https://googleapis.dev/python/aiplatform/latest/aiplatform.html\n",
    "\"\"\"\n",
    "from typing import Generic, TypeVar, Optional, List, Dict\n",
    "from pydantic import BaseModel, parse_file_as\n",
    "from google.cloud import aiplatform\n",
    "from pathlib import Path\n",
    "import os\n",
    "class Setup(BaseModel): \n",
    "    project_id: str \n",
    "    # It is not the \"NAME\", but the unique ID assigned during creation of the project\n",
    "    \n",
    "    region: Optional[str] \n",
    "    # This is optional, stores where the model is to be saved. Currently forecasting is \n",
    "    # available only in us-central1 and eu-west4\n",
    "    \n",
    "    staging_bucket_name: str \n",
    "    # Any bucket that Vertex AI can use for its training.\n",
    "    \n",
    "    def init_ai(self):\n",
    "        aiplatform.init(project=self.project_id, staging_bucket=self.staging_bucket_name, location=self.region)\n",
    "\n",
    "class Dataset(BaseModel):\n",
    "    # Dataset requirements: There are several subtleties involved in preparing the dataset if it does not exist\n",
    "    # Please refer https://cloud.google.com/vertex-ai/docs/datasets/prepare-tabular#import-source for best practices and data preparation.\n",
    "    \n",
    "    exists: bool \n",
    "    # Argument if dataset already created (Vertex AI first converts csv/bigquery to \"DATASET\")\n",
    "    \n",
    "    source: str \n",
    "    # If exists is True: pass the ID of the dataset. E.g. projects/242781379053/locations/us-central1/datasets/8236001799018905600 or 8236001799018905600.\n",
    "    #If False: pass the url (either gcs or bq) where the dataset is present. \n",
    "    \n",
    "    gcs: Optional[bool] = True \n",
    "    # This option is required if exists != True. In that case, this option specifies whether \n",
    "    # dataset is bigquery or gcs.\n",
    "    \n",
    "    display_name: Optional[str] \n",
    "    # This option is required if exists != True. You can specify the display name that appears\n",
    "    # in the vertex AI console using this method.\n",
    "    \n",
    "    def init_dataset(self):\n",
    "        if self.exists:\n",
    "            return aiplatform.TimeSeriesDataset(dataset_name = self.source)\n",
    "        else:\n",
    "            if not self.gcs:\n",
    "                return aiplatform.datasets.TimeSeriesDataset.create(display_name=self.display_name,\n",
    "                                                                    bq_source=self.source)\n",
    "            else:\n",
    "                return aiplatform.datasets.TimeSeriesDataset.create(display_name=self.display_name, gcs_source=self.source)\n",
    "\n",
    "class TrainingJob(BaseModel):\n",
    "    display_name: str \n",
    "    # Required: Displays the name of the training job in the console\n",
    "    \n",
    "    time_column: str \n",
    "    # A timestamp field. Tells the date/time of that datapoint\n",
    "    \n",
    "    target_column: str \n",
    "    # The column of the dataset that needs to be forecast\n",
    "    \n",
    "    time_series_identifier_column: str \n",
    "    # Column showing different time series, e.g. user ID during BP prediction \n",
    "    \n",
    "    available_at_forecast_columns: List[str] \n",
    "    # Columns that are available during forecasting (e.g. time_column)\n",
    "    \n",
    "    unavailable_at_forecast_columns: List[str] \n",
    "    # Columns that are unavailable during forecasting (e.g. )\n",
    "    \n",
    "    data_granularity_unit: str \n",
    "    # Possible values: minute, hour, day, week, month, year, defines the time period of the provided time series.\n",
    "    \n",
    "    data_granularity_count = 1 \n",
    "    # By default 1 unless specified in JSON (allowed:  1, 5, 10, 15, or 30). Only allowed to specify if data_granularity_unit is minutes.\n",
    "    # Example if data_granularity_count = 5 then 5 minutes between each datapoint in the time series.\n",
    "    \n",
    "    forecast_horizon: int # The number of points in the future to be predicted\n",
    "    \n",
    "    context_window : int \n",
    "    # The number of points used in the past for prediction. The recommended value is [forecast_horizon, 10*forecast_horizon].\n",
    "    \n",
    "    time_series_attribute_columns: Optional[List[str]] = None \n",
    "    # Attributes that are fixed for a fixed time_series_identifier_column. For example: a user's age may\n",
    "    # be considered fixed and thus belongs to the time_series_attribute_columns.\n",
    "    \n",
    "    predefined_split_column: Optional[str] = None \n",
    "    # Vertex AI Default split: https://cloud.google.com/vertex-ai/docs/datasets/prepare-tabular#import-source\n",
    "    # If custom split required then each datapoint needs to be classified as TRAIN, VAL and TEST. Care needs to be taken to ensure no information leakage during training.\n",
    "    \n",
    "    optimization_objective: Optional[str] = None \n",
    "    # Options \"minimize-rmse\", \"minimize-mae\", \"minimize-rmspe\", \"minimize-wape-mae\", \"minimize-quantile-loss\" \n",
    "    # ref: https://googleapis.dev/python/aiplatform/latest/aiplatform.html\n",
    "    \n",
    "    column_transformations: Optional[List[Dict[str, Dict[str, str]]]] = None \n",
    "    # available transformations: numeric, text, categorical, timestamp\n",
    "    \n",
    "    weight_column: Optional[str] = None \n",
    "    # Specifies weight of the row/datapoint for prediction accuracy. Similar to Weighted LR.\n",
    "    \n",
    "    quantiles: Optional[List[float]] = None \n",
    "    # This option is only required if loss is quantile loss\n",
    "    \n",
    "    budget_milli_node_hours: Optional[int] = None \n",
    "    # By Default 1000 hours. Defines training budget, based on this more models are explored\n",
    "    \n",
    "    model_display_name: Optional[str] = None \n",
    "    # Name of the model after training (optional).\n",
    "    \n",
    "    export_evaluated_data_items: False \n",
    "    # If False, then does not export Test results. If True, test results exported as bigquery database.\n",
    "    \n",
    "    export_evaluated_data_items_bigquery_destination_uri: Optional[str] = None\n",
    "    # Optional, if export_evaluated_data_items is True, exports as bigquery. Format of string: bq://<project_id>:<dataset_id>:<table>\n",
    "    # If not specified and export_evaluated_data_items is True, creates new table of format: \n",
    "    # <project_id>:export_evaluated_examples_<model_name>_<yyyy_MM_dd'T'HH_mm_ss_SSS'Z'>.evaluated_examples\n",
    "    \n",
    "    def create_and_run(self, ds):\n",
    "        job = aiplatform.AutoMLForecastingTrainingJob(\n",
    "            display_name=self.display_name,\n",
    "            optimization_objective=self.optimization_objective,    \n",
    "            column_transformations = self.column_transformations\n",
    "            )\n",
    "        model = job.run(\n",
    "            dataset=ds,\n",
    "            target_column=self.target_column,\n",
    "            time_column=self.time_column,\n",
    "            time_series_identifier_column=self.time_series_identifier_column,\n",
    "            available_at_forecast_columns=self.available_at_forecast_columns,\n",
    "            unavailable_at_forecast_columns=self.unavailable_at_forecast_columns,\n",
    "            time_series_attribute_columns=self.time_series_attribute_columns,\n",
    "            forecast_horizon=self.forecast_horizon,\n",
    "            context_window=self.context_window,\n",
    "            data_granularity_unit=self.data_granularity_unit,\n",
    "            data_granularity_count=self.data_granularity_count,\n",
    "            weight_column=self.weight_column,\n",
    "            budget_milli_node_hours=self.budget_milli_node_hours,\n",
    "            model_display_name=self.model_display_name, \n",
    "            predefined_split_column_name=self.predefined_split_column,\n",
    "            export_evaluated_data_items=self.export_evaluated_data_items,\n",
    "            export_evaluated_data_items_bigquery_destination_uri=self.export_evaluated_data_items_bigquery_destination_uri\n",
    "        )\n",
    "    \n",
    "class Main(BaseModel):\n",
    "    setup: Setup\n",
    "    dataset: Dataset\n",
    "    training_job: TrainingJob\n",
    "    def run(self):\n",
    "        os.system(\"gcloud config set project \" + self.setup.project_id)\n",
    "        self.setup.init_ai()\n",
    "        self.training_job.create_and_run(self.dataset.init_dataset())\n",
    "\n",
    "filepath = \"training_automl_config.json\" # Filepath to config file for training\n",
    "main_func = parse_file_as(Main, filepath)\n",
    "main_func.run()"
   ],
   "outputs": [],
   "metadata": {
    "id": "fa93634e"
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Training_automl.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}